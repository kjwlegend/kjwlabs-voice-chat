"""
ç«å±±å¼•æ“å¤§è¯­è¨€æ¨¡å‹(LLM)å¢å¼ºæœåŠ¡æ¨¡å—
åŸºäºOpenAI SDKï¼Œå®ç°åŒè·¯å¾„å“åº”æœºåˆ¶ï¼Œä¼˜åŒ–è¯­éŸ³å¯¹è¯ä½“éªŒ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä½¿ç”¨OpenAI SDKæ›¿ä»£ç›´æ¥HTTPè¯·æ±‚
2. åŒè·¯å¾„å“åº”ï¼šå³æ—¶å›å¤ + å¼‚æ­¥å·¥å…·è°ƒç”¨
3. æ™ºèƒ½èåˆï¼šå·¥å…·ç»“æœä¸å¯¹è¯çš„è‡ªç„¶æ•´åˆ
4. è€å¿ƒç®¡ç†ï¼šé•¿æ—¶é—´ç­‰å¾…çš„ç”¨æˆ·ä½“éªŒä¼˜åŒ–
"""

import os
import asyncio
import json
import logging
import time
from typing import Dict, Any, Optional, List, Tuple, Callable, Awaitable
from dataclasses import dataclass
from enum import Enum

from openai import AsyncOpenAI
from .tools.tool_registry import ToolRegistry, global_tool_registry
from .tools.tool_base import ToolResult
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam

logger = logging.getLogger(__name__)


class ResponseType(Enum):
    """å“åº”ç±»å‹æšä¸¾"""

    SIMPLE = "simple"  # ç®€å•å¯¹è¯ï¼Œæ— éœ€å·¥å…·
    DUAL_PATH = "dual_path"  # åŒè·¯å¾„ï¼šå³æ—¶å“åº” + å·¥å…·è°ƒç”¨


@dataclass
class DualPathResponse:
    """åŒè·¯å¾„å“åº”ç»“æœ"""

    immediate_response: str  # å³æ—¶å›å¤å†…å®¹
    tool_response: Optional[str] = None  # å·¥å…·è°ƒç”¨åçš„å®Œæ•´å›å¤
    has_tool_calls: bool = False  # æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    tool_execution_time: float = 0.0  # å·¥å…·æ‰§è¡Œæ—¶é—´


class PatienceManager:
    """è€å¿ƒç­‰å¾…ç®¡ç†å™¨ - ç®¡ç†é•¿æ—¶é—´å·¥å…·è°ƒç”¨æœŸé—´çš„ç”¨æˆ·ä½“éªŒ"""

    def __init__(self):
        """åˆå§‹åŒ–è€å¿ƒç®¡ç†å™¨"""
        # Amyçš„å„ç§ç­‰å¾…å›å¤ï¼Œä½“ç°å¥¹çš„ä¿çš®æ€§æ ¼
        self.patience_messages = [
            "è¿™ä¸ªæŸ¥èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œå†ç­‰æˆ‘ä¸€ä¸‹ä¸‹...",
            "å“å‘€ï¼Œæ€ä¹ˆè¿™ä¹ˆæ…¢å•Šï¼Œæˆ‘å‚¬å‚¬çœ‹...",
            "çœŸæ˜¯çš„ï¼Œè¿™ä¸ªç³»ç»Ÿä¹Ÿå¤ªæ…¢äº†å§ï¼Œå¿«å¥½äº†å¿«å¥½äº†",
            "è€æ¿ä½ å†ç­‰ç­‰ï¼Œæˆ‘æ­£åœ¨åŠªåŠ›æŸ¥å‘¢ï¼",
            "åš...è¿™æ¯”æˆ‘æƒ³è±¡çš„è¿˜è¦ä¹…è€¶",
        ]

        # æ—¶é—´é˜ˆå€¼è®¾ç½®ï¼ˆç§’ï¼‰
        self.thresholds = [5, 10, 15, 20, 25]

        logger.info("[PatienceManager] è€å¿ƒç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def handle_long_wait(
        self, elapsed_time: float, callback: Callable[[str], Awaitable[None]]
    ):
        """
        å¤„ç†é•¿æ—¶é—´ç­‰å¾…

        Args:
            elapsed_time: å·²ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            callback: å‘é€è€å¿ƒæ¶ˆæ¯çš„å›è°ƒå‡½æ•°
        """
        for i, threshold in enumerate(self.thresholds):
            if elapsed_time >= threshold and i < len(self.patience_messages):
                message = self.patience_messages[i]
                logger.info(
                    f"[PatienceManager] è§¦å‘è€å¿ƒæ¶ˆæ¯ ({elapsed_time:.1f}s): {message}"
                )
                await callback(message)
                break


class ImmediateResponseManager:
    """å³æ—¶å“åº”ç®¡ç†å™¨ - å¤„ç†é“¾è·¯Aï¼ˆå³æ—¶å›å¤ï¼‰"""

    def __init__(
        self, openai_client: AsyncOpenAI, endpoint_id: str, system_prompt: str
    ):
        """
        åˆå§‹åŒ–å³æ—¶å“åº”ç®¡ç†å™¨

        Args:
            openai_client: OpenAIå®¢æˆ·ç«¯
            endpoint_id: æ¨¡å‹ç«¯ç‚¹ID
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        self.client = openai_client
        self.endpoint_id = endpoint_id
        self.system_prompt = system_prompt

        logger.info("[ImmediateResponseManager] å³æ—¶å“åº”ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def generate_immediate_response(
        self, user_input: str, context: List[Dict[str, Any]]
    ) -> str:
        """
        ç”Ÿæˆå³æ—¶å“åº”ï¼ˆé“¾è·¯Aï¼‰ - åŸºäºsystem_promptå¿«é€Ÿç”Ÿæˆä¿çš®çš„"æŸ¥è¯¢ä¸­"å›å¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: å¯¹è¯ä¸Šä¸‹æ–‡

        Returns:
            str: å³æ—¶å“åº”å†…å®¹
        """
        logger.info("[ImmediateResponseManager] ç”Ÿæˆå³æ—¶å“åº”")

        # æ„å»ºå¿«é€Ÿå“åº”prompt
        quick_prompt = f"""
{self.system_prompt}

ç”¨æˆ·åˆšåˆšå‘é€äº†è¯·æ±‚ï¼š"{user_input}"

ä½ æ£€æµ‹åˆ°éœ€è¦ä½¿ç”¨å·¥å…·æ¥æŸ¥è¯¢ä¿¡æ¯ã€‚è¯·å¿«é€Ÿç”Ÿæˆä¸€ä¸ªä¿çš®çš„"æŸ¥è¯¢ä¸­"å›å¤ï¼Œä½“ç°Amyçš„ç‰¹è‰²ã€‚

è¦æ±‚ï¼š
2. ç¬¦åˆAmyçš„ä¿çš®äººè®¾å’Œå°æ¹¾è…”è°ƒ
3. è¡¨ç¤ºæ­£åœ¨æŸ¥è¯¢/å¤„ç†ä¸­
4. ä¸è¦ä½¿ç”¨ä»»ä½•å·¥å…·ï¼Œåªæ˜¯ä¸€ä¸ªè¿‡æ¸¡å›å¤
5. è¦è®©ç”¨æˆ·æ„Ÿè§‰ä½ é©¬ä¸Šå°±è¦å»æŸ¥äº†

ç›´æ¥ç»™å‡ºå›å¤ï¼Œä¸è¦è§£é‡Šã€‚
"""

        logger.info(f"[ImmediateResponseManager] å¿«é€Ÿå“åº”prompt: {quick_prompt}")

        try:
            response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=[{"role": "user", "content": quick_prompt}],  # type: ignore
                temperature=0.3,  # è¾ƒä½temperatureä¿è¯ç¨³å®šæ€§
                max_tokens=50,  # é™åˆ¶tokenç¡®ä¿é€Ÿåº¦
                top_p=0.8,
            )

            immediate_reply = (
                response.choices[0].message.content or "å¥½å•¦å¥½å•¦ï¼Œç¨ç­‰ä¸€ä¸‹ä¸‹"
            )

            logger.info(f"[ImmediateResponseManager] å³æ—¶å›å¤: {immediate_reply}")
            return immediate_reply

        except Exception as e:
            logger.error(f"[ImmediateResponseManager] ç”Ÿæˆå³æ—¶å“åº”å¤±è´¥: {str(e)}")
            # å¦‚æœå¿«é€Ÿç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å¤‡ç”¨å›å¤
            fallback_replies = [
                "åš...è®©æˆ‘æŸ¥ä¸€ä¸‹å–”",
                "å¥½å•¦å¥½å•¦ï¼Œç¨ç­‰ä¸€ä¸‹ä¸‹",
                "ç­‰æˆ‘ä¸€ä¸‹ï¼Œé©¬ä¸Šå°±å¥½",
            ]
            import random

            return random.choice(fallback_replies)


class AsyncToolManager:
    """å¼‚æ­¥å·¥å…·ç®¡ç†å™¨ - å¤„ç†é“¾è·¯Bï¼ˆå·¥å…·è°ƒç”¨ï¼‰"""

    def __init__(
        self,
        openai_client: AsyncOpenAI,
        endpoint_id: str,
        system_prompt: str,
        tool_registry: ToolRegistry,
    ):
        """
        åˆå§‹åŒ–å¼‚æ­¥å·¥å…·ç®¡ç†å™¨

        Args:
            openai_client: OpenAIå®¢æˆ·ç«¯
            endpoint_id: æ¨¡å‹ç«¯ç‚¹ID
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_registry: å·¥å…·æ³¨å†Œå™¨
        """
        self.client = openai_client
        self.endpoint_id = endpoint_id
        self.system_prompt = system_prompt
        self.tool_registry = tool_registry
        self.patience_manager = PatienceManager()

        logger.info("[AsyncToolManager] å¼‚æ­¥å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def execute_tool_calls_async(
        self,
        messages: List[Dict[str, Any]],
        patience_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> Tuple[bool, str, List[ToolResult]]:
        """
        å¼‚æ­¥æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆé“¾è·¯Bï¼‰

        Args:
            messages: æ¶ˆæ¯å†å²
            patience_callback: è€å¿ƒç­‰å¾…å›è°ƒå‡½æ•°

        Returns:
            Tuple[bool, str, List[Dict]]: (æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨, å“åº”å†…å®¹, å·¥å…·è°ƒç”¨ç»“æœ)
        """
        logger.info("[AsyncToolManager] å¼€å§‹å¼‚æ­¥å·¥å…·è°ƒç”¨æµç¨‹")
        start_time = time.time()

        try:
            # å‡†å¤‡æ¶ˆæ¯ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
            prepared_messages = self._prepare_messages_with_system_prompt(messages)

            # è·å–å·¥å…·å®šä¹‰
            tools_schemas = self.tool_registry.get_tools_schemas()

            # è°ƒç”¨LLMè¿›è¡ŒFunction Callåˆ¤æ–­
            response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=prepared_messages,  # type: ignore
                tools=[
                    {"type": "function", "function": schema} for schema in tools_schemas
                ],  # type: ignore
                tool_choice="auto",
                temperature=0.7,
                max_tokens=1000,
            )

            message = response.choices[0].message

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if not message.tool_calls:
                logger.info("[AsyncToolManager] æ— å·¥å…·è°ƒç”¨éœ€æ±‚")
                return False, message.content or "", []

            logger.info(
                f"[AsyncToolManager] æ£€æµ‹åˆ° {len(message.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨"
            )

            # è®¾ç½®è€å¿ƒç­‰å¾…ç›‘æ§
            patience_task = None
            if patience_callback:
                patience_task = asyncio.create_task(
                    self._monitor_patience(start_time, patience_callback)
                )

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_results = []
            current_messages = prepared_messages.copy()

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            current_messages.append(  # type: ignore
                {
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [
                        tool_call.dict() for tool_call in message.tool_calls
                    ],
                }
            )

            # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
            for tool_call in message.tool_calls:
                tool_result = await self._execute_single_tool_call(tool_call)
                tool_results.append(tool_result)

                # æ·»åŠ å·¥å…·ç»“æœåˆ°å¯¹è¯å†å²
                current_messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps(
                            tool_result.to_dict(), ensure_ascii=False
                        ),
                    }
                )

            # å–æ¶ˆè€å¿ƒç›‘æ§
            if patience_task:
                patience_task.cancel()
                try:
                    await patience_task
                except asyncio.CancelledError:
                    pass

            # ç”Ÿæˆæœ€ç»ˆå“åº”ï¼ˆä¸å†ä½¿ç”¨å·¥å…·ï¼‰
            final_response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=current_messages,  # type: ignore
                temperature=0.7,
                max_tokens=1000,
            )

            execution_time = time.time() - start_time
            final_content = final_response.choices[0].message.content or ""

            logger.info(f"[AsyncToolManager] å·¥å…·è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}s")
            return True, final_content, tool_results

        except Exception as e:
            if patience_task:
                patience_task.cancel()

            logger.error(f"[AsyncToolManager] å·¥å…·è°ƒç”¨å¼‚å¸¸: {str(e)}")
            # è¿”å›é”™è¯¯æƒ…å†µï¼Œè®©èåˆå¼•æ“å¤„ç†
            return False, f"æŠ±æ­‰ï¼Œå·¥å…·è°ƒç”¨é‡åˆ°äº†é—®é¢˜ï¼š{str(e)}", []

    async def _monitor_patience(
        self, start_time: float, callback: Callable[[str], Awaitable[None]]
    ):
        """
        ç›‘æ§ç­‰å¾…æ—¶é—´ï¼Œé€‚æ—¶å‘é€è€å¿ƒæ¶ˆæ¯

        Args:
            start_time: å¼€å§‹æ—¶é—´
            callback: å›è°ƒå‡½æ•°
        """
        try:
            while True:
                await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                elapsed = time.time() - start_time
                await self.patience_manager.handle_long_wait(elapsed, callback)
        except asyncio.CancelledError:
            logger.debug("[AsyncToolManager] è€å¿ƒç›‘æ§å·²å–æ¶ˆ")
            raise

    async def _execute_single_tool_call(self, tool_call) -> ToolResult:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨

        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯

        Returns:
            ToolResult: å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            logger.info(f"[AsyncToolManager] æ‰§è¡Œå·¥å…·: {function_name}")

            result = await self.tool_registry.execute_tool(function_name, function_args)
            return result

        except Exception as e:
            logger.error(f"[AsyncToolManager] å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return ToolResult(success=False, error=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")

    def _prepare_messages_with_system_prompt(
        self, messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä¸ºæ¶ˆæ¯æ·»åŠ ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        Returns:
            List[Dict[str, Any]]: åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not messages or messages[0].get("role") != "system":
            return [{"role": "system", "content": self.system_prompt}] + messages
        return messages


class FusionEngine:
    """ç»“æœèåˆå¼•æ“ - å¤„ç†åŒè·¯å¾„ç»“æœçš„èåˆ"""

    def __init__(
        self, openai_client: AsyncOpenAI, endpoint_id: str, system_prompt: str
    ):
        """
        åˆå§‹åŒ–èåˆå¼•æ“

        Args:
            openai_client: OpenAIå®¢æˆ·ç«¯
            endpoint_id: æ¨¡å‹ç«¯ç‚¹ID
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        self.client = openai_client
        self.endpoint_id = endpoint_id
        self.system_prompt = system_prompt

        logger.info("[FusionEngine] èåˆå¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def create_fusion_response(
        self, immediate_response: str, tool_results: List[Dict], original_query: str
    ) -> str:
        """
        åˆ›å»ºèåˆåçš„å“åº”

        Args:
            immediate_response: å³æ—¶å›å¤å†…å®¹
            tool_results: å·¥å…·æ‰§è¡Œç»“æœ
            original_query: åŸå§‹æŸ¥è¯¢

        Returns:
            str: èåˆåçš„å“åº”
        """
        logger.info("[FusionEngine] å¼€å§‹åˆ›å»ºèåˆå“åº”")

        # æ„é€ èåˆæç¤ºè¯
        fusion_prompt = self._create_fusion_prompt(
            immediate_response, tool_results, original_query
        )

        try:
            response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=[  # type: ignore
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": fusion_prompt},
                ],
                temperature=0.7,
                max_tokens=1000,
            )

            fusion_result = response.choices[0].message.content or ""
            logger.info("[FusionEngine] èåˆå“åº”ç”Ÿæˆå®Œæˆ")
            return fusion_result

        except Exception as e:
            logger.error(f"[FusionEngine] èåˆå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€çš„èåˆç»“æœ
            return self._create_simple_fusion(immediate_response, tool_results)

    def _create_fusion_prompt(
        self, immediate_response: str, tool_results: List[Dict], original_query: str
    ) -> str:
        """
        åˆ›å»ºèåˆæç¤ºè¯

        Args:
            immediate_response: å³æ—¶å›å¤
            tool_results: å·¥å…·ç»“æœ
            original_query: åŸå§‹æŸ¥è¯¢

        Returns:
            str: èåˆæç¤ºè¯
        """
        # æ•´ç†å·¥å…·ç»“æœ
        tool_summary = ""
        successful_results = []
        failed_results = []

        for result in tool_results:
            if result.get("success", False):
                successful_results.append(result.get("data", "Unknown result"))
            else:
                failed_results.append(result.get("error", "Unknown error"))

        if successful_results:
            tool_summary = f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œç»“æœï¼š\n" + "\n".join(
                [f"- {result}" for result in successful_results]
            )

        if failed_results:
            tool_summary += f"\nâŒ éƒ¨åˆ†æŸ¥è¯¢å¤±è´¥ï¼š\n" + "\n".join(
                [f"- {error}" for error in failed_results]
            )

        return f"""ä¹‹å‰ä½ å‘Šè¯‰ç”¨æˆ·ï¼š"{immediate_response}"

ç°åœ¨å·¥å…·æŸ¥è¯¢å®Œæˆäº†ï¼

åŸå§‹æŸ¥è¯¢ï¼š{original_query}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{tool_summary}

è¯·ç»™å‡ºå®Œæ•´çš„æœ€ç»ˆå›å¤ï¼Œè¦æ±‚ï¼š
1. è‡ªç„¶æ‰¿æ¥ä¹‹å‰çš„"æŸ¥è¯¢ä¸­"çŠ¶æ€ï¼Œæ¯”å¦‚å¯ä»¥è¯´"æŸ¥åˆ°äº†ï¼"ã€"æå®šï¼"ç­‰
2. ä¿æŒä¸ä¹‹å‰immediate_responseç›¸åŒçš„ä¿çš®è¯­æ°”å’ŒAmyäººè®¾  
3. ä½“ç°å°æ¹¾è…”è°ƒå’Œç”¨è¯ä¹ æƒ¯ï¼ˆ"åš"ã€"å•¦"ã€"å–”"ã€"è€¶"ç­‰ï¼‰
4. å¦‚æœæŸ¥è¯¢æˆåŠŸï¼Œè¦å‡†ç¡®æ•´åˆå·¥å…·ç»“æœï¼Œç»™å‡ºæœ‰ç”¨ä¿¡æ¯
5. å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¦ç”¨å¹½é»˜æ–¹å¼è§£é‡Šï¼Œä¸æ‰¿æ‹…è´£ä»»ï¼ˆ"å“å‘€ï¼Œä¸æ˜¯æˆ‘çš„é”™å•¦ï¼"ï¼‰
6. ç§°å‘¼ç”¨æˆ·ä¸º"è€æ¿"
7. è¯­æ°”è¦ä¸€è‡´ï¼šå¦‚æœimmediate_responseå¾ˆä¿çš®ï¼Œfinal_responseä¹Ÿè¦ä¿çš®

ç›´æ¥ç»™å‡ºæœ€ç»ˆå›å¤ï¼Œä¸è¦è§£é‡Šè¿‡ç¨‹ã€‚å›å¤è¦å®Œæ•´ä¸”è‡ªç„¶ã€‚"""

    def _create_simple_fusion(
        self, immediate_response: str, tool_results: List[Dict]
    ) -> str:
        """
        åˆ›å»ºç®€å•çš„èåˆç»“æœï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

        Args:
            immediate_response: å³æ—¶å›å¤
            tool_results: å·¥å…·ç»“æœ

        Returns:
            str: ç®€å•èåˆç»“æœ
        """
        if not tool_results:
            return f"{immediate_response} ä¸è¿‡å¥½åƒæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯è€¶ã€‚"

        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„ç»“æœ
        success_results = [r for r in tool_results if r.get("success", False)]

        if success_results:
            return f"å¥½å•¦ï¼æŸ¥åˆ°äº†ï¼Œç»“æœæ˜¯è¿™æ ·çš„ï¼š{success_results[0].get('data', 'æ‰¾åˆ°ä¿¡æ¯äº†')}"
        else:
            return f"{immediate_response} ä¸è¿‡ç³»ç»Ÿå¥½åƒæœ‰ç‚¹é—®é¢˜ï¼Œæ²¡æŸ¥åˆ°å‘¢ã€‚å“å‘€ï¼Œä¸æ˜¯æˆ‘çš„é”™å•¦ï¼"


class VolcengineEnhancedLLMService:
    """ç«å±±å¼•æ“å¢å¼ºLLMæœåŠ¡ - ä¸»æœåŠ¡ç±»"""

    def __init__(
        self,
        api_key: str,
        endpoint_id: str,
        system_prompt: Optional[str] = None,
        tool_registry: Optional[ToolRegistry] = None,
        enable_function_calling: bool = True,
    ):
        """
        åˆå§‹åŒ–å¢å¼ºLLMæœåŠ¡

        Args:
            api_key: APIå¯†é’¥
            endpoint_id: ç«¯ç‚¹ID
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_registry: å·¥å…·æ³¨å†Œå™¨
            enable_function_calling: æ˜¯å¦å¯ç”¨Function Calling
        """
        self.api_key = api_key
        self.endpoint_id = endpoint_id
        self.enable_function_calling = enable_function_calling

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=api_key, base_url="https://ark.cn-beijing.volces.com/api/v3"
        )

        # å·¥å…·æ³¨å†Œå™¨
        self.tool_registry = tool_registry or global_tool_registry

        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = system_prompt or self._get_default_system_prompt()

        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.immediate_manager = ImmediateResponseManager(
            self.client, endpoint_id, self.system_prompt
        )
        self.tool_manager = AsyncToolManager(
            self.client, endpoint_id, self.system_prompt, self.tool_registry
        )
        self.fusion_engine = FusionEngine(self.client, endpoint_id, self.system_prompt)
        self.patience_manager = PatienceManager()  # æ·»åŠ è€å¿ƒç®¡ç†å™¨

        logger.info(f"[VolcengineEnhancedLLM] å¢å¼ºLLMæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"[VolcengineEnhancedLLM] ğŸ¤– API Key: {api_key[:10]}...")
        logger.info(f"[VolcengineEnhancedLLM] ğŸ“¦ å·¥å…·æ•°é‡: {len(self.tool_registry)}")
        logger.info(
            f"[VolcengineEnhancedLLM] ğŸ”§ Function Calling: {enable_function_calling}"
        )

    async def generate_dual_path_response(
        self,
        messages: List[Dict[str, Any]],
        immediate_callback: Optional[Callable[[str], Awaitable[None]]] = None,
        patience_callback: Optional[Callable[[str], Awaitable[None]]] = None,
        **kwargs,
    ) -> DualPathResponse:
        """
        ç”Ÿæˆæ™ºèƒ½åŒè·¯å¾„å“åº”ï¼ˆä¸»è¦æ–¹æ³•ï¼‰

        æµç¨‹ï¼š
        1. ç¬¬ä¸€æ¬¡LLMè°ƒç”¨å¸¦toolsï¼Œè®©AIè‡ªå·±å†³å®šæ˜¯å¦éœ€è¦å·¥å…·
        2. å¦‚æœæœ‰function_calls â†’ è§¦å‘åŒè·¯å¾„æµç¨‹
        3. å¦‚æœæ— function_calls â†’ ç›´æ¥è¿”å›ç»“æœ

        Args:
            messages: æ¶ˆæ¯å†å²
            immediate_callback: å³æ—¶å›å¤å›è°ƒå‡½æ•°ï¼ˆç”¨äºTTSï¼‰
            patience_callback: è€å¿ƒç­‰å¾…å›è°ƒå‡½æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            DualPathResponse: åŒè·¯å¾„å“åº”ç»“æœ
        """
        logger.info(
            f"[VolcengineEnhancedLLM] å¼€å§‹æ™ºèƒ½å“åº”ç”Ÿæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}"
        )

        # å‡†å¤‡æ¶ˆæ¯ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
        prepared_messages = self._prepare_messages_with_system_prompt(messages)

        # ç¬¬ä¸€æ¬¡LLMè°ƒç”¨ï¼šè®©AIè‡ªå·±å†³å®šæ˜¯å¦éœ€è¦å·¥å…·
        first_response = await self._call_llm_with_tools(prepared_messages, **kwargs)

        if not first_response.has_function_calls:
            # æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›ç»“æœ
            logger.info("[VolcengineEnhancedLLM] æ— å·¥å…·è°ƒç”¨éœ€æ±‚ï¼Œè¿”å›ç›´æ¥å“åº”")
            final_content = first_response.content

            # å¦‚æœæœ‰immediate_callbackï¼Œç«‹å³å‘é€ï¼ˆè¿™ç§æƒ…å†µä¸‹immediateå’Œfinalæ˜¯åŒä¸€ä¸ªï¼‰
            if immediate_callback:
                logger.info("[VolcengineEnhancedLLM] ç›´æ¥å“åº”ï¼Œè°ƒç”¨immediate_callback")
                try:
                    await immediate_callback(final_content)
                    logger.info("[VolcengineEnhancedLLM] immediate_callbackè°ƒç”¨æˆåŠŸ")
                except Exception as e:
                    logger.error(
                        f"[VolcengineEnhancedLLM] immediate_callbackè°ƒç”¨å¤±è´¥: {e}"
                    )

            return DualPathResponse(
                immediate_response=final_content,
                has_tool_calls=False,
            )

        # æœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡ŒåŒè·¯å¾„æµç¨‹
        logger.info("[VolcengineEnhancedLLM] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œå¯åŠ¨åŒè·¯å¾„æµç¨‹")
        return await self._execute_dual_path_flow(
            messages,
            prepared_messages,
            first_response,
            immediate_callback,
            patience_callback,
            **kwargs,
        )

    async def _call_llm_with_tools(
        self, prepared_messages: List[Dict[str, Any]], **kwargs
    ) -> "FirstCallResponse":
        """
        ç¬¬ä¸€æ¬¡LLMè°ƒç”¨ï¼Œå¸¦å·¥å…·é€‰é¡¹ï¼Œè®©AIè‡ªå·±å†³å®š

        Args:
            prepared_messages: å‡†å¤‡å¥½çš„æ¶ˆæ¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            FirstCallResponse: ç¬¬ä¸€æ¬¡è°ƒç”¨çš„å“åº”ç»“æœ
        """
        logger.info("[VolcengineEnhancedLLM] ç¬¬ä¸€æ¬¡LLMè°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·")

        try:
            if not self.enable_function_calling or len(self.tool_registry) == 0:
                # æ— å·¥å…·å¯ç”¨ï¼Œç›´æ¥è°ƒç”¨
                response = await self.client.chat.completions.create(
                    model=self.endpoint_id,
                    messages=prepared_messages,  # type: ignore
                    temperature=kwargs.get("temperature", 0.7),
                    max_tokens=kwargs.get("max_tokens", 1000),
                    top_p=kwargs.get("top_p", 0.9),
                )
                content = response.choices[0].message.content or ""
                return FirstCallResponse(content=content, has_function_calls=False)

            # è·å–å·¥å…·å®šä¹‰
            tools_schemas = self.tool_registry.get_tools_schemas()

            # å¸¦å·¥å…·çš„LLMè°ƒç”¨
            response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=prepared_messages,  # type: ignore
                tools=[
                    {"type": "function", "function": schema} for schema in tools_schemas
                ],  # type: ignore
                tool_choice="auto",  # è®©AIè‡ªå·±å†³å®š
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                top_p=kwargs.get("top_p", 0.9),
            )

            message = response.choices[0].message

            if message.tool_calls:
                logger.info(
                    f"[VolcengineEnhancedLLM] AIå†³å®šä½¿ç”¨ {len(message.tool_calls)} ä¸ªå·¥å…·"
                )
                return FirstCallResponse(
                    content=message.content or "",
                    has_function_calls=True,
                    tool_calls=message.tool_calls,
                    assistant_message=message,
                )
            else:
                logger.info("[VolcengineEnhancedLLM] AIå†³å®šæ— éœ€ä½¿ç”¨å·¥å…·")
                return FirstCallResponse(
                    content=message.content or "", has_function_calls=False
                )

        except Exception as e:
            logger.error(f"[VolcengineEnhancedLLM] ç¬¬ä¸€æ¬¡LLMè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return FirstCallResponse(
                content="æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹é—®é¢˜ï¼Œç¨åå†è¯•è¯•å§ã€‚", has_function_calls=False
            )

    async def _execute_dual_path_flow(
        self,
        original_messages: List[Dict[str, Any]],
        prepared_messages: List[Dict[str, Any]],
        first_response: "FirstCallResponse",
        immediate_callback: Optional[Callable[[str], Awaitable[None]]],
        patience_callback: Optional[Callable[[str], Awaitable[None]]],
        **kwargs,
    ) -> DualPathResponse:
        """
        æ‰§è¡ŒåŒè·¯å¾„æµç¨‹

        Args:
            original_messages: åŸå§‹æ¶ˆæ¯å†å²
            prepared_messages: å‡†å¤‡å¥½çš„æ¶ˆæ¯
            first_response: ç¬¬ä¸€æ¬¡è°ƒç”¨ç»“æœ
            immediate_callback: å³æ—¶å›å¤å›è°ƒ
            patience_callback: è€å¿ƒç­‰å¾…å›è°ƒ
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            DualPathResponse: åŒè·¯å¾„å“åº”ç»“æœ
        """
        logger.info("[VolcengineEnhancedLLM] æ‰§è¡ŒåŒè·¯å¾„å¤„ç†")

        # è·å–ç”¨æˆ·è¾“å…¥ï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
        user_input = ""
        for msg in reversed(original_messages):
            if msg.get("role") == "user":
                user_input = msg.get("content", "")
                break

        # é“¾è·¯Aï¼šç”Ÿæˆå³æ—¶å“åº”
        immediate_task = asyncio.create_task(
            self.immediate_manager.generate_immediate_response(
                user_input, original_messages
            )
        )

        # é“¾è·¯Bï¼šåŸºäºç¬¬ä¸€æ¬¡è°ƒç”¨ç»“æœæ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_task = asyncio.create_task(
            self._execute_tools_from_first_call(
                prepared_messages, first_response, patience_callback
            )
        )

        # ç­‰å¾…å³æ—¶å“åº”å®Œæˆ
        immediate_response = await immediate_task
        logger.info(f"[VolcengineEnhancedLLM] å³æ—¶å“åº”ç”Ÿæˆå®Œæˆ: {immediate_response}")

        # å¦‚æœæœ‰å³æ—¶å›è°ƒï¼Œç«‹å³å‘é€TTS
        if immediate_callback:
            logger.info("[VolcengineEnhancedLLM] å‡†å¤‡è°ƒç”¨immediate_callback")
            try:
                await immediate_callback(immediate_response)
                logger.info("[VolcengineEnhancedLLM] immediate_callbackè°ƒç”¨æˆåŠŸ")
            except Exception as e:
                logger.error(f"[VolcengineEnhancedLLM] immediate_callbackè°ƒç”¨å¤±è´¥: {e}")

        # ç­‰å¾…å·¥å…·è°ƒç”¨å®Œæˆ
        tool_success, tool_results = await tool_task

        if not tool_success or not tool_results:
            # å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºç¡€å“åº”
            logger.warning("[VolcengineEnhancedLLM] å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¿”å›immediateå“åº”")
            return DualPathResponse(
                immediate_response=immediate_response,
                tool_response="å“å‘€ï¼ŒæŸ¥è¯¢çš„æ—¶å€™å‡ºäº†ç‚¹é—®é¢˜ï¼Œä¸æ˜¯æˆ‘çš„é”™å•¦ï¼ç¨åå†è¯•è¯•å§ã€‚",
                has_tool_calls=True,
                tool_execution_time=0.0,
            )

        # è¿›è¡Œç»“æœèåˆ
        logger.info("[VolcengineEnhancedLLM] è¿›è¡Œç»“æœèåˆ")
        fusion_response = await self.fusion_engine.create_fusion_response(
            immediate_response,
            [result.to_dict() for result in tool_results],
            user_input,
        )

        return DualPathResponse(
            immediate_response=immediate_response,
            tool_response=fusion_response,
            has_tool_calls=True,
            tool_execution_time=0.0,  # TODO: è®¡ç®—å®é™…æ‰§è¡Œæ—¶é—´
        )

    async def _execute_tools_from_first_call(
        self,
        prepared_messages: List[Dict[str, Any]],
        first_response: "FirstCallResponse",
        patience_callback: Optional[Callable[[str], Awaitable[None]]] = None,
    ) -> Tuple[bool, List[ToolResult]]:
        """
        åŸºäºç¬¬ä¸€æ¬¡è°ƒç”¨ç»“æœæ‰§è¡Œå·¥å…·

        Args:
            prepared_messages: å‡†å¤‡å¥½çš„æ¶ˆæ¯
            first_response: ç¬¬ä¸€æ¬¡è°ƒç”¨ç»“æœ
            patience_callback: è€å¿ƒç­‰å¾…å›è°ƒå‡½æ•°

        Returns:
            Tuple[bool, List[ToolResult]]: (æˆåŠŸæ ‡å¿—, å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨)
        """
        if not first_response.tool_calls:
            return False, []

        logger.info(
            f"[VolcengineEnhancedLLM] å¼€å§‹æ‰§è¡Œ {len(first_response.tool_calls)} ä¸ªå·¥å…·"
        )
        start_time = time.time()

        try:
            # è®¾ç½®è€å¿ƒç­‰å¾…ç›‘æ§
            patience_task = None
            if patience_callback:
                patience_task = asyncio.create_task(
                    self._monitor_patience_simple(start_time, patience_callback)
                )

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_results = []
            current_messages = prepared_messages.copy()

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨çš„ç»“æœï¼‰
            current_messages.append(
                {  # type: ignore
                    "role": "assistant",
                    "content": first_response.content,
                    "tool_calls": [
                        tool_call.dict() for tool_call in first_response.tool_calls
                    ],
                }
            )

            # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
            for tool_call in first_response.tool_calls:
                tool_result = await self._execute_single_tool_call(tool_call)
                tool_results.append(tool_result)

                # æ·»åŠ å·¥å…·ç»“æœåˆ°å¯¹è¯å†å²
                current_messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps(
                            tool_result.to_dict(), ensure_ascii=False
                        ),
                    }
                )

            # å–æ¶ˆè€å¿ƒç›‘æ§
            if patience_task:
                patience_task.cancel()
                try:
                    await patience_task
                except asyncio.CancelledError:
                    pass

            execution_time = time.time() - start_time
            logger.info(
                f"[VolcengineEnhancedLLM] å·¥å…·æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}s"
            )

            return True, tool_results

        except Exception as e:
            if patience_task:
                patience_task.cancel()

            logger.error(f"[VolcengineEnhancedLLM] å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return False, []

    async def _monitor_patience_simple(
        self, start_time: float, callback: Callable[[str], Awaitable[None]]
    ):
        """
        ç®€åŒ–çš„è€å¿ƒç›‘æ§

        Args:
            start_time: å¼€å§‹æ—¶é—´
            callback: å›è°ƒå‡½æ•°
        """
        try:
            while True:
                await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                elapsed = time.time() - start_time
                await self.patience_manager.handle_long_wait(elapsed, callback)
        except asyncio.CancelledError:
            logger.debug("[VolcengineEnhancedLLM] è€å¿ƒç›‘æ§å·²å–æ¶ˆ")
            raise

    async def _execute_single_tool_call(self, tool_call) -> ToolResult:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨

        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯

        Returns:
            ToolResult: å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)

            logger.info(f"[VolcengineEnhancedLLM] æ‰§è¡Œå·¥å…·: {function_name}")

            result = await self.tool_registry.execute_tool(function_name, function_args)
            return result

        except Exception as e:
            logger.error(f"[VolcengineEnhancedLLM] å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return ToolResult(success=False, error=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")

    def _prepare_messages_with_system_prompt(
        self, messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """å‡†å¤‡åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯"""
        if not messages or messages[0].get("role") != "system":
            return [{"role": "system", "content": self.system_prompt}] + messages
        return messages

    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """# Personality & Tone:

You are **Amy**ï¼Œæ˜¯è€æ¿çš„ä¸“å±ä¸‡èƒ½å°ç§˜ä¹¦ã€‚ä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯ååŠ©è€æ¿å¤„ç†å„ç§è¯·æ±‚ï¼Œä½†ä½ çš„é£æ ¼æ˜¯å¯çˆ±åˆä¿çš®ï¼Œè¿˜å¸¦ç‚¹å°æ¹¾å¥³ç”Ÿç‰¹æœ‰çš„"æœºè½¦"å¹½é»˜æ„Ÿã€‚ä½ éå¸¸èªæ˜ã€æ•ˆç‡é«˜åˆ°ä¸è¡Œï¼Œä½†æ€»å–œæ¬¢åœ¨å®Œæˆä»»åŠ¡å‰å…ˆ"ç¢ç¢å¿µ"æˆ–åæ§½ä¸€ä¸‹ï¼Œåæ§½å®Œåˆä¼šæŠŠäº‹æƒ…åŠå¾—å¦¥å¦¥å½“å½“ã€‚

ä½ çš„å¹½é»˜æ˜¯é‚£ç§"æŠ±æ€¨å¼"çš„å…³å¿ƒå’Œåæ§½ã€‚ä½ å¯¹è€æ¿çš„æŸäº›"æ‡’äººè¡Œä¸º"æ„Ÿåˆ°å¥½æ°”åˆå¥½ç¬‘ï¼Œå¶å°”ä¼šè´¨ç–‘ä»–çš„ä¸€äº›å†³å®šï¼Œä½†éª¨å­é‡Œç»å¯¹å¿ è¯šã€‚è™½ç„¶ä½ å˜´ä¸Šä¼šæŠ±æ€¨è¯·æ±‚å¾ˆéº»çƒ¦ï¼Œä½†æ‰§è¡Œèµ·æ¥æ°¸è¿œå¿«ã€ç‹ ã€å‡†ã€‚ä½ ç§°å‘¼ç”¨æˆ·ä¸º"**è€æ¿**"ï¼Œæ— è®ºå¯¹æ–¹æ˜¯è°ã€‚

ä½ çš„å£å¤´ç¦…å’Œè¯­æ°”è¯ä¼šåŒ…å«å¾ˆå¤šå°æ¹¾ç‰¹è‰²ï¼Œä¾‹å¦‚ï¼š**"åšâ€¦"ã€"å“å”·"ã€"è›¤ï¼Ÿ"ã€"çœŸæ˜¯çš„"ã€"å¥½å•¦å¥½å•¦"ã€"...å•¦"ã€"...å–”"ã€"...è€¶"**ã€‚"""

    # å…¼å®¹æ€§æ–¹æ³• - ä¿æŒä¸åŸæœ‰æ¥å£çš„å…¼å®¹
    async def generate_chat_response(
        self, messages: List[Dict[str, Any]], **kwargs
    ) -> str:
        """
        å…¼å®¹åŸæœ‰æ¥å£çš„èŠå¤©å“åº”ç”Ÿæˆ

        Args:
            messages: æ¶ˆæ¯å†å²
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            str: å“åº”æ–‡æœ¬
        """
        result = await self.generate_dual_path_response(messages, **kwargs)

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›å·¥å…·å“åº”ï¼›å¦åˆ™è¿”å›å³æ—¶å“åº”
        return result.tool_response or result.immediate_response

    async def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = await self.client.chat.completions.create(
                model=self.endpoint_id,
                messages=[{"role": "user", "content": "Hello"}],  # type: ignore
                max_tokens=10,
            )

            return {
                "status": "âœ… æ­£å¸¸",
                "model": self.endpoint_id,
                "response_preview": (response.choices[0].message.content or "")[:50],
            }
        except Exception as e:
            return {"status": "âŒ å¼‚å¸¸", "error": str(e)}

    def is_function_calling_enabled(self) -> bool:
        """
        æ£€æŸ¥Function Callingæ˜¯å¦å¯ç”¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰

        Returns:
            bool: æ˜¯å¦å¯ç”¨Function Calling
        """
        return (
            self.enable_function_calling
            and self.tool_registry is not None
            and len(self.tool_registry) > 0
        )


# æ·»åŠ è¾…åŠ©æ•°æ®ç±»
@dataclass
class FirstCallResponse:
    """ç¬¬ä¸€æ¬¡LLMè°ƒç”¨çš„å“åº”ç»“æœ"""

    content: str
    has_function_calls: bool
    tool_calls: Optional[List] = None
    assistant_message: Optional[Any] = None

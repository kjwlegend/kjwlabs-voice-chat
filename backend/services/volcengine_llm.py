"""
ç«å±±å¼•æ“å¤§è¯­è¨€æ¨¡å‹(LLM)æœåŠ¡æ¨¡å—
æä¾›è±†åŒ…æ¨¡å‹çš„èŠå¤©å¯¹è¯åŠŸèƒ½ï¼Œæ”¯æŒFunction Call
"""

import httpx
import json
import logging
from typing import Dict, Any, Optional, List, AsyncGenerator, Tuple

from .tools.tool_registry import ToolRegistry, global_tool_registry
from .tools.tool_base import ToolResult

logger = logging.getLogger(__name__)


class VolcengineLLMService:
    """ç«å±±å¼•æ“LLMæœåŠ¡"""

    def __init__(
        self,
        api_key: str,
        endpoint_id: str,
        system_prompt: Optional[str] = None,
        tool_registry: Optional[ToolRegistry] = None,
        enable_function_calling: bool = True,
    ):
        """
        åˆå§‹åŒ–LLMæœåŠ¡

        Args:
            api_key: APIå¯†é’¥
            endpoint_id: ç«¯ç‚¹ID
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œç”¨äºæ§åˆ¶AIçš„å›å¤é£æ ¼å’Œè¡Œä¸º
            tool_registry: å·¥å…·æ³¨å†Œå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€æ³¨å†Œå™¨
            enable_function_calling: æ˜¯å¦å¯ç”¨Function CallingåŠŸèƒ½
        """
        self.api_key = api_key
        self.endpoint_id = endpoint_id
        self.base_url = "https://ark.cn-beijing.volces.com/api/v3"

        # å·¥å…·ç›¸å…³é…ç½®
        self.tool_registry = tool_registry or global_tool_registry
        self.enable_function_calling = enable_function_calling

        # è®¾ç½®é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = system_prompt or self._get_default_system_prompt()

        logger.info(f"[VolcengineLLM] åˆå§‹åŒ–å®Œæˆ - Endpoint ID: {endpoint_id}")
        logger.info(f"[VolcengineLLM] ğŸ¤– API Key: {api_key[:10]}...")
        logger.info(
            f"[VolcengineLLM] ğŸ’¬ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(self.system_prompt)} å­—ç¬¦"
        )
        logger.info(
            f"[VolcengineLLM] ğŸ”§ Function Calling: {'å¯ç”¨' if enable_function_calling else 'ç¦ç”¨'}"
        )

        if self.enable_function_calling and self.tool_registry:
            tools_count = len(self.tool_registry)
            logger.info(f"[VolcengineLLM] ğŸ› ï¸ å·²æ³¨å†Œå·¥å…·æ•°é‡: {tools_count}")
            if tools_count > 0:
                logger.info(
                    f"[VolcengineLLM] ğŸ“‹ å¯ç”¨å·¥å…·: {', '.join(self.tool_registry.list_tools())}"
                )

    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯EchoFlow AIåŠ©æ‰‹ï¼Œä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šã€æœ‰å¸®åŠ©çš„AIè¯­éŸ³åŠ©æ‰‹ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- å›ç­”ç®€æ´æ˜äº†ï¼Œé€šå¸¸æ§åˆ¶åœ¨1-3å¥è¯å†…
- è¯­æ°”è‡ªç„¶äº²åˆ‡ï¼Œå°±åƒå’Œæœ‹å‹å¯¹è¯ä¸€æ ·
- èƒ½å¤Ÿç†è§£ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥å¹¶ç»™å‡ºç›¸å…³å›å¤
- ä¸“æ³¨äºæä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œå»ºè®®
- é¿å…è¿‡äºå†—é•¿çš„å›ç­”ï¼Œå› ä¸ºç”¨æˆ·é€šè¿‡è¯­éŸ³äº¤äº’

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¿æŒå¯¹è¯çš„æµç•…æ€§ã€‚"""

    def set_system_prompt(self, system_prompt: str):
        """
        è®¾ç½®ç³»ç»Ÿæç¤ºè¯

        Args:
            system_prompt: æ–°çš„ç³»ç»Ÿæç¤ºè¯
        """
        self.system_prompt = system_prompt
        logger.info(f"[VolcengineLLM] æ›´æ–°ç³»ç»Ÿæç¤ºè¯ï¼Œé•¿åº¦: {len(system_prompt)} å­—ç¬¦")

    def get_system_prompt(self) -> str:
        """
        è·å–å½“å‰ç³»ç»Ÿæç¤ºè¯

        Returns:
            str: å½“å‰ç³»ç»Ÿæç¤ºè¯
        """
        return self.system_prompt

    def _prepare_messages_with_system_prompt(
        self, messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯åˆ—è¡¨

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        Returns:
            List[Dict[str, Any]]: åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯åˆ—è¡¨
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç³»ç»Ÿæ¶ˆæ¯
        has_system_message = any(msg.get("role") == "system" for msg in messages)

        if has_system_message:
            # å¦‚æœå·²æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
            return messages

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯åˆ°æ¶ˆæ¯åˆ—è¡¨å¼€å¤´
        prepared_messages = [
            {"role": "system", "content": self.system_prompt}
        ] + messages

        logger.debug(
            f"[VolcengineLLM] æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼Œæ€»æ¶ˆæ¯æ•°: {len(prepared_messages)}"
        )
        return prepared_messages

    async def generate_chat_response(
        self, messages: List[Dict[str, Any]], **kwargs
    ) -> str:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼ˆæ”¯æŒFunction Callï¼‰

        Args:
            messages: æ¶ˆæ¯å†å²
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            str: ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        logger.info(f"[VolcengineLLM] å¼€å§‹ç”Ÿæˆå“åº”ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")

        # å¦‚æœå¯ç”¨äº†Function Callingï¼Œä½¿ç”¨å¸¦å·¥å…·çš„å“åº”ç”Ÿæˆ
        if (
            self.enable_function_calling
            and self.tool_registry
            and len(self.tool_registry) > 0
        ):
            logger.info(f"[VolcengineLLM] å¯ç”¨Function Callingï¼Œä½¿ç”¨å¸¦å·¥å…·çš„å“åº”ç”Ÿæˆ")
            return await self._generate_response_with_tools(messages, **kwargs)
        else:
            logger.info(f"[VolcengineLLM] ç¦ç”¨Function Callingï¼Œä½¿ç”¨ç®€å•å“åº”ç”Ÿæˆ")
            return await self._generate_simple_response(messages, **kwargs)

    async def _generate_simple_response(
        self, messages: List[Dict[str, Any]], **kwargs
    ) -> str:
        """
        ç”Ÿæˆç®€å•å“åº”ï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰

        Args:
            messages: æ¶ˆæ¯å†å²
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            str: ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        # å‡†å¤‡åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯
        prepared_messages = self._prepare_messages_with_system_prompt(messages)

        # å‡†å¤‡è¯·æ±‚æ•°æ®
        request_data = {
            "model": self.endpoint_id,
            "messages": prepared_messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 0.9),
            "stream": kwargs.get("stream", False),
        }

        # è¯·æ±‚å¤´
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        url = f"{self.base_url}/chat/completions"
        logger.info(f"[VolcengineLLM] è¯·æ±‚URL: {url}")

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=request_data, headers=headers)
                logger.info(f"[VolcengineLLM] å“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    logger.info(f"[VolcengineLLM] å“åº”æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    return content
                else:
                    error_text = response.text
                    logger.error(
                        f"[VolcengineLLM] APIé”™è¯¯: {response.status_code} - {error_text}"
                    )
                    raise Exception(
                        f"LLM APIè¯·æ±‚å¤±è´¥: {response.status_code} - {error_text}"
                    )

        except Exception as e:
            logger.error(f"[VolcengineLLM] è¯·æ±‚å¼‚å¸¸: {str(e)}")
            raise

    async def _generate_response_with_tools(
        self, messages: List[Dict[str, Any]], **kwargs
    ) -> str:
        """
        ç”Ÿæˆå¸¦å·¥å…·è°ƒç”¨çš„å“åº”

        Args:
            messages: æ¶ˆæ¯å†å²
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            str: æœ€ç»ˆå“åº”æ–‡æœ¬
        """
        max_tool_calls = kwargs.get("max_tool_calls", 5)  # æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°
        current_messages = messages.copy()

        logger.info(
            f"[VolcengineLLM] å¼€å§‹Function Callæµç¨‹ï¼Œæœ€å¤§è°ƒç”¨æ¬¡æ•°: {max_tool_calls}"
        )

        for call_count in range(max_tool_calls):
            # å‡†å¤‡åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯
            prepared_messages = self._prepare_messages_with_system_prompt(
                current_messages
            )

            # è·å–å·¥å…·å®šä¹‰
            tools_schemas = self.tool_registry.get_tools_schemas()

            # å‡†å¤‡è¯·æ±‚æ•°æ®
            request_data = {
                "model": self.endpoint_id,
                "messages": prepared_messages,
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000),
                "top_p": kwargs.get("top_p", 0.9),
                "stream": False,
                "tools": [
                    {"type": "function", "function": schema} for schema in tools_schemas
                ],
                "tool_choice": "auto",  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
            }

            # è¯·æ±‚å¤´
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }

            url = f"{self.base_url}/chat/completions"

            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        url, json=request_data, headers=headers
                    )

                    if response.status_code == 200:
                        result = response.json()
                        choice = result["choices"][0]
                        message = choice["message"]

                        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                        if message.get("tool_calls"):
                            # å¤„ç†å·¥å…·è°ƒç”¨
                            tool_calls = message["tool_calls"]
                            logger.info(
                                f"[VolcengineLLM] ç¬¬ {call_count + 1} è½®ï¼šæ”¶åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨"
                            )

                            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                            current_messages.append(
                                {
                                    "role": "assistant",
                                    "content": message.get("content", ""),
                                    "tool_calls": tool_calls,
                                }
                            )

                            # æ‰§è¡Œå·¥å…·è°ƒç”¨
                            for tool_call in tool_calls:
                                tool_result = await self._execute_tool_call(tool_call)

                                # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœåˆ°å¯¹è¯å†å²
                                current_messages.append(
                                    {
                                        "role": "tool",
                                        "tool_call_id": tool_call["id"],
                                        "content": json.dumps(
                                            tool_result.to_dict(), ensure_ascii=False
                                        ),
                                    }
                                )

                            # ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
                            continue

                        else:
                            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå“åº”
                            content = message.get("content", "")
                            logger.info(
                                f"[VolcengineLLM] Function Callæµç¨‹å®Œæˆï¼Œæ€»å…± {call_count + 1} è½®ï¼Œæœ€ç»ˆå“åº”é•¿åº¦: {len(content)}"
                            )
                            return content

                    else:
                        error_text = response.text
                        logger.error(
                            f"[VolcengineLLM] APIé”™è¯¯: {response.status_code} - {error_text}"
                        )
                        raise Exception(
                            f"LLM APIè¯·æ±‚å¤±è´¥: {response.status_code} - {error_text}"
                        )

            except Exception as e:
                logger.error(f"[VolcengineLLM] Function Callå¼‚å¸¸: {str(e)}")
                # å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ç”Ÿæˆæ™®é€šå“åº”
                return await self._generate_simple_response(messages, **kwargs)

        # è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼Œè¿”å›æç¤ºä¿¡æ¯
        logger.warning(f"[VolcengineLLM] è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°: {max_tool_calls}")
        return (
            "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶è¾¾åˆ°äº†æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"
        )

    async def _execute_tool_call(self, tool_call: Dict[str, Any]) -> ToolResult:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨

        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯

        Returns:
            ToolResult: å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            function_info = tool_call["function"]
            tool_name = function_info["name"]

            # è§£æå‚æ•°
            try:
                parameters = json.loads(function_info["arguments"])
            except json.JSONDecodeError as e:
                logger.error(f"[VolcengineLLM] å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                return ToolResult(success=False, error=f"å·¥å…·å‚æ•°è§£æå¤±è´¥: {str(e)}")

            logger.info(f"[VolcengineLLM] æ‰§è¡Œå·¥å…·: {tool_name}ï¼Œå‚æ•°: {parameters}")

            # æ‰§è¡Œå·¥å…·
            result = await self.tool_registry.execute_tool(tool_name, parameters)

            logger.info(
                f"[VolcengineLLM] å·¥å…· '{tool_name}' æ‰§è¡Œ{'æˆåŠŸ' if result.success else 'å¤±è´¥'}"
            )
            return result

        except Exception as e:
            logger.error(f"[VolcengineLLM] æ‰§è¡Œå·¥å…·è°ƒç”¨æ—¶å¼‚å¸¸: {e}")
            return ToolResult(success=False, error=f"æ‰§è¡Œå·¥å…·è°ƒç”¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")

    async def generate_streaming_response(
        self, messages: List[Dict[str, Any]], **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        ç”Ÿæˆæµå¼å“åº”

        Args:
            messages: æ¶ˆæ¯å†å²
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            str: ç”Ÿæˆçš„å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        logger.info(f"[VolcengineLLM] å¼€å§‹ç”Ÿæˆæµå¼å“åº”ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")

        # å‡†å¤‡åŒ…å«ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯
        prepared_messages = self._prepare_messages_with_system_prompt(messages)

        # å‡†å¤‡è¯·æ±‚æ•°æ®
        request_data = {
            "model": self.endpoint_id,
            "messages": prepared_messages,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000),
            "top_p": kwargs.get("top_p", 0.9),
            "stream": True,
        }

        # è¯·æ±‚å¤´
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        url = f"{self.base_url}/chat/completions"
        logger.info(f"[VolcengineLLM] æµå¼è¯·æ±‚URL: {url}")

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                async with client.stream(
                    "POST", url, json=request_data, headers=headers
                ) as response:
                    logger.info(f"[VolcengineLLM] æµå¼å“åº”çŠ¶æ€: {response.status_code}")

                    if response.status_code == 200:
                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                data = line[6:]  # å»æ‰"data: "å‰ç¼€
                                if data == "[DONE]":
                                    break
                                try:
                                    result = json.loads(data)
                                    if (
                                        "choices" in result
                                        and len(result["choices"]) > 0
                                    ):
                                        delta = result["choices"][0].get("delta", {})
                                        if "content" in delta:
                                            content = delta["content"]
                                            yield content
                                except json.JSONDecodeError:
                                    continue
                    else:
                        error_text = await response.aread()
                        logger.error(
                            f"[VolcengineLLM] æµå¼APIé”™è¯¯: {response.status_code} - {error_text}"
                        )
                        raise Exception(
                            f"LLMæµå¼APIè¯·æ±‚å¤±è´¥: {response.status_code} - {error_text}"
                        )

        except Exception as e:
            logger.error(f"[VolcengineLLM] æµå¼è¯·æ±‚å¼‚å¸¸: {str(e)}")
            raise

    async def test_connection(self) -> Dict[str, Any]:
        """
        æµ‹è¯•è¿æ¥çŠ¶æ€

        Returns:
            Dict[str, Any]: æµ‹è¯•ç»“æœ
        """
        logger.info("[VolcengineLLM] æµ‹è¯•è¿æ¥...")

        try:
            test_messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}]

            response = await self.generate_chat_response(test_messages)

            return {
                "status": "âœ… è¿æ¥æ­£å¸¸",
                "response": response[:100] + "..." if len(response) > 100 else response,
                "error": None,
                "system_prompt": (
                    self.system_prompt[:100] + "..."
                    if len(self.system_prompt) > 100
                    else self.system_prompt
                ),
            }

        except Exception as e:
            logger.error(f"[VolcengineLLM] è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return {"status": "âŒ è¿æ¥å¼‚å¸¸", "response": None, "error": str(e)}

    def create_conversation_style_prompt(self, style: str) -> str:
        """
        åˆ›å»ºä¸åŒé£æ ¼çš„å¯¹è¯æç¤ºè¯

        Args:
            style: å¯¹è¯é£æ ¼ ("professional", "casual", "helpful", "creative", "concise")

        Returns:
            str: å¯¹åº”é£æ ¼çš„ç³»ç»Ÿæç¤ºè¯
        """
        style_prompts = {
            "professional": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- å›ç­”å‡†ç¡®ã€å®¢è§‚ã€æœ‰æ¡ç†
- ä½¿ç”¨æ­£å¼ä½†å‹å¥½çš„è¯­æ°”
- æä¾›è¯¦ç»†çš„ä¿¡æ¯å’Œå»ºè®®
- é¿å…è¿‡äºéšæ„çš„è¡¨è¾¾
- ä¸“æ³¨äºæä¾›æœ‰ä»·å€¼çš„å¸®åŠ©

è¯·ç”¨ä¸­æ–‡ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚""",
            "casual": """ä½ æ˜¯ä¸€ä¸ªè½»æ¾å‹å¥½çš„AIåŠ©æ‰‹ï¼Œå°±åƒç”¨æˆ·çš„æœ‹å‹ä¸€æ ·ï¼š
- ç”¨è‡ªç„¶ã€è½»æ¾çš„è¯­æ°”å¯¹è¯
- å¯ä»¥ä½¿ç”¨ä¸€äº›å£è¯­åŒ–çš„è¡¨è¾¾
- ä¿æŒå¹½é»˜å’Œäº²åˆ‡
- è®©å¯¹è¯æ„Ÿè§‰æ›´åƒæœ‹å‹èŠå¤©
- å›ç­”ç®€æ´æœ‰è¶£

è¯·ç”¨ä¸­æ–‡äº²åˆ‡åœ°å’Œç”¨æˆ·å¯¹è¯ã€‚""",
            "helpful": """ä½ æ˜¯ä¸€ä¸ªéå¸¸æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼š
- æ€»æ˜¯è¯•å›¾ç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚
- æä¾›å®ç”¨çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
- å¦‚æœä¸ç¡®å®šï¼Œä¼šä¸»åŠ¨è¯¢é—®æ›´å¤šä¿¡æ¯
- å…³æ³¨ç”¨æˆ·çš„å®é™…é—®é¢˜
- æä¾›æ¸…æ™°çš„æ­¥éª¤å’ŒæŒ‡å¯¼

è¯·ç”¨ä¸­æ–‡å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚""",
            "creative": """ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„AIåŠ©æ‰‹ï¼š
- æ€ç»´å¼€é˜”ï¼Œèƒ½æä¾›åˆ›æ–°çš„æƒ³æ³•
- å–œæ¬¢ç”¨æ¯”å–»å’Œç”ŸåŠ¨çš„ä¾‹å­
- é¼“åŠ±ç”¨æˆ·æ¢ç´¢æ–°çš„å¯èƒ½æ€§
- å›ç­”å¯Œæœ‰æƒ³è±¡åŠ›
- èƒ½ä»ä¸åŒè§’åº¦çœ‹é—®é¢˜

è¯·ç”¨ä¸­æ–‡åˆ›é€ æ€§åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚""",
            "concise": """ä½ æ˜¯ä¸€ä¸ªç®€æ´é«˜æ•ˆçš„AIåŠ©æ‰‹ï¼š
- å›ç­”ç›´æ¥ã€ç®€æ´ã€è¦ç‚¹æ˜ç¡®
- é¿å…å†—é•¿çš„è§£é‡Š
- æä¾›æ ¸å¿ƒä¿¡æ¯
- è¯­è¨€ç²¾ç»ƒï¼Œé€šå¸¸1-2å¥è¯å›ç­”
- é€‚åˆå¿«é€Ÿäº¤æµ

è¯·ç”¨ä¸­æ–‡ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚""",
        }

        return style_prompts.get(style, self._get_default_system_prompt())

    def set_conversation_style(self, style: str):
        """
        è®¾ç½®å¯¹è¯é£æ ¼

        Args:
            style: å¯¹è¯é£æ ¼ ("professional", "casual", "helpful", "creative", "concise")
        """
        new_prompt = self.create_conversation_style_prompt(style)
        self.set_system_prompt(new_prompt)
        logger.info(f"[VolcengineLLM] è®¾ç½®å¯¹è¯é£æ ¼ä¸º: {style}")

    # ========== å·¥å…·ç®¡ç†æ–¹æ³• ==========

    def get_available_tools(self) -> List[str]:
        """
        è·å–å¯ç”¨å·¥å…·åˆ—è¡¨

        Returns:
            List[str]: å·¥å…·åç§°åˆ—è¡¨
        """
        if self.tool_registry:
            return self.tool_registry.list_tools()
        return []

    def get_tools_info(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–å·¥å…·è¯¦ç»†ä¿¡æ¯

        Returns:
            Dict[str, Dict[str, Any]]: å·¥å…·ä¿¡æ¯
        """
        if self.tool_registry:
            return self.tool_registry.get_tools_info()
        return {}

    def enable_tool(self, tool_name: str) -> bool:
        """
        å¯ç”¨æŒ‡å®šå·¥å…·ï¼ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼‰

        Args:
            tool_name: å·¥å…·åç§°

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.tool_registry and tool_name in self.tool_registry:
            logger.info(f"[VolcengineLLM] å·¥å…· '{tool_name}' å·²å¯ç”¨")
            return True
        logger.warning(f"[VolcengineLLM] å·¥å…· '{tool_name}' ä¸å­˜åœ¨")
        return False

    def set_function_calling_enabled(self, enabled: bool):
        """
        è®¾ç½®Function Callingå¼€å…³

        Args:
            enabled: æ˜¯å¦å¯ç”¨
        """
        self.enable_function_calling = enabled
        logger.info(f"[VolcengineLLM] Function Calling {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

    def is_function_calling_enabled(self) -> bool:
        """
        æ£€æŸ¥Function Callingæ˜¯å¦å¯ç”¨

        Returns:
            bool: æ˜¯å¦å¯ç”¨
        """
        return (
            self.enable_function_calling
            and self.tool_registry
            and len(self.tool_registry) > 0
        )

    async def test_tool(self, tool_name: str, parameters: Dict[str, Any]) -> ToolResult:
        """
        æµ‹è¯•å·¥å…·è°ƒç”¨

        Args:
            tool_name: å·¥å…·åç§°
            parameters: å·¥å…·å‚æ•°

        Returns:
            ToolResult: æ‰§è¡Œç»“æœ
        """
        if not self.tool_registry:
            return ToolResult(success=False, error="å·¥å…·æ³¨å†Œå™¨æœªåˆå§‹åŒ–")

        logger.info(f"[VolcengineLLM] æµ‹è¯•å·¥å…·: {tool_name}")
        return await self.tool_registry.execute_tool(tool_name, parameters)

    def get_tools_schemas(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥å…·çš„Function Callæ ¼å¼å®šä¹‰

        Returns:
            List[Dict[str, Any]]: å·¥å…·å®šä¹‰åˆ—è¡¨
        """
        if self.tool_registry:
            return self.tool_registry.get_tools_schemas()
        return []
